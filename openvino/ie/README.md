# Using the Intel OpenVINO Inference Engine

The Intel OpenVINO Inference Engine is a set of libraries for executing convolutional neural networks.

GoCV support for the Intel OpenVINO Inference Engine will be able to be found here in the "gocv.io/x/gocv/openvino/ie" package.

## Roadmap

Support in GoCV for the Intel OpenVINO Inference Engine is still under development, and is not yet in a usable state.

- [ ] InferenceEnginePlugin - work started, can load plugin
- [X] InferenceEngine::CNNNetReader
- [ ] InferenceEngine::CNNNetwork  - work started
- [ ] InferenceEngine::InputsDataMap
- [ ] InferenceEngine::InputInfo
- [ ] InferenceEngine::OutputsDataMap
- [ ] InferenceEngine::ExecutableNetwork
- [ ] InferenceEngine::InferRequest
- [ ] InferenceEngine::Blob
